{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0fbdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 13:17:06.152655: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-10 13:17:09.037650: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-10 13:17:09.037686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-10 13:17:09.160716: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 13:17:09.418593: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv1D, \\\n",
    "    AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout, Softmax\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow import keras\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def fro_norm(w):\n",
    "    return K.sqrt(K.sum(K.square(K.abs(w))))\n",
    "\n",
    "\n",
    "def cust_reg(w):\n",
    "    m = K.dot(K.transpose(w), w) - np.eye(w.shape)\n",
    "    return fro_norm(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1d3e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700000, 60)\n",
      "(700000, 90)\n",
      "(18400, 60)\n",
      "(18400, 90)\n"
     ]
    }
   ],
   "source": [
    "# SNR\n",
    "Training_Data = genfromtxt('Training_Data.csv', delimiter=\",\")\n",
    "Training_Data = np.array(Training_Data)\n",
    "#Training_Data = Training_Data[...,None]\n",
    "print(Training_Data.shape)\n",
    "\n",
    "Training_Labels = genfromtxt('Training_Labels.csv', delimiter=\",\")\n",
    "Training_Labels = np.array(Training_Labels)\n",
    "#Training_Labels = Training_Labels[...,None]\n",
    "print(Training_Labels.shape)\n",
    "\n",
    "Testing_Data = genfromtxt('Testing_Data.csv', delimiter=\",\")\n",
    "Testing_Data = np.array(Testing_Data)\n",
    "#Testing_Data = Testing_Data[...,None]\n",
    "print(Testing_Data.shape)\n",
    "\n",
    "Testing_Labels = genfromtxt('Testing_Labels.csv', delimiter=\",\")\n",
    "Testing_Labels = np.array(Testing_Labels)\n",
    "#Testing_Labels = Testing_Labels[...,None]\n",
    "print(Testing_Labels.shape)\n",
    "\n",
    "\n",
    "indices_1 = np.arange(Training_Data.shape[0])\n",
    "np.random.shuffle(indices_1)\n",
    "Training_Data = Training_Data[indices_1]\n",
    "Training_Labels = Training_Labels[indices_1]\n",
    "\n",
    "indices_2 = np.arange(Testing_Data.shape[0])\n",
    "np.random.shuffle(indices_2)\n",
    "Testing_Data = Testing_Data[indices_2]\n",
    "Testing_Labels = Testing_Labels[indices_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200d246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block_1(X):\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Dense(480, activation=None, kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(240, activation='relu', kernel_initializer=glorot_uniform(seed=0))(X) #252\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(120, activation='relu', kernel_initializer=glorot_uniform(seed=0), bias_regularizer='l2', kernel_regularizer='l2')(X) #126\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(60, activation='relu', kernel_initializer=glorot_uniform(seed=0))(X) #252\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def ResNet(input_shape=(60,)):\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "    X = Dense(60, activation=None, kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Adding identity block\n",
    "    X = identity_block_1(X)\n",
    "    X = identity_block_1(X)\n",
    "    X = identity_block_1(X)\n",
    "    X = identity_block_1(X)\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(90, activation='sigmoid', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df57fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 13:18:21.516832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'),\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb50138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 13:18:34.108109: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fc1675e5520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-10 13:18:34.108125: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-12-10 13:18:34.130942: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-10 13:18:34.466203: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733829514.809428 3080088 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386/1386 [==============================] - 31s 12ms/step - loss: 0.3502 - tp: 2752543.0000 - fp: 2032320.0000 - tn: 54100692.0000 - fn: 3484457.0000 - accuracy: 0.9115 - precision: 0.5753 - recall: 0.4413 - auc: 0.9479 - val_loss: 0.1275 - val_tp: 37671.0000 - val_fp: 16095.0000 - val_tn: 550905.0000 - val_fn: 25329.0000 - val_accuracy: 0.9342 - val_precision: 0.7006 - val_recall: 0.5980 - val_auc: 0.9705\n",
      "Epoch 2/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0907 - tp: 4551719.0000 - fp: 890615.0000 - tn: 55242368.0000 - fn: 1685281.0000 - accuracy: 0.9587 - precision: 0.8364 - recall: 0.7298 - auc: 0.9872 - val_loss: 0.0608 - val_tp: 54716.0000 - val_fp: 5873.0000 - val_tn: 561127.0000 - val_fn: 8284.0000 - val_accuracy: 0.9775 - val_precision: 0.9031 - val_recall: 0.8685 - val_auc: 0.9955\n",
      "Epoch 3/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0462 - tp: 5737868.0000 - fp: 369105.0000 - tn: 55763888.0000 - fn: 499132.0000 - accuracy: 0.9861 - precision: 0.9396 - recall: 0.9200 - auc: 0.9975 - val_loss: 0.0263 - val_tp: 61269.0000 - val_fp: 1717.0000 - val_tn: 565283.0000 - val_fn: 1731.0000 - val_accuracy: 0.9945 - val_precision: 0.9727 - val_recall: 0.9725 - val_auc: 0.9993\n",
      "Epoch 4/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0268 - tp: 6014483.0000 - fp: 174353.0000 - tn: 55958596.0000 - fn: 222517.0000 - accuracy: 0.9936 - precision: 0.9718 - recall: 0.9643 - auc: 0.9989 - val_loss: 0.0171 - val_tp: 61851.0000 - val_fp: 939.0000 - val_tn: 566061.0000 - val_fn: 1149.0000 - val_accuracy: 0.9967 - val_precision: 0.9850 - val_recall: 0.9818 - val_auc: 0.9996\n",
      "Epoch 5/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0206 - tp: 6072274.0000 - fp: 132955.0000 - tn: 56000056.0000 - fn: 164726.0000 - accuracy: 0.9952 - precision: 0.9786 - recall: 0.9736 - auc: 0.9992 - val_loss: 0.0144 - val_tp: 62085.0000 - val_fp: 860.0000 - val_tn: 566140.0000 - val_fn: 915.0000 - val_accuracy: 0.9972 - val_precision: 0.9863 - val_recall: 0.9855 - val_auc: 0.9997\n",
      "Epoch 6/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0176 - tp: 6096758.0000 - fp: 115641.0000 - tn: 56017376.0000 - fn: 140242.0000 - accuracy: 0.9959 - precision: 0.9814 - recall: 0.9775 - auc: 0.9993 - val_loss: 0.0114 - val_tp: 62360.0000 - val_fp: 753.0000 - val_tn: 566247.0000 - val_fn: 640.0000 - val_accuracy: 0.9978 - val_precision: 0.9881 - val_recall: 0.9898 - val_auc: 0.9998\n",
      "Epoch 7/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0159 - tp: 6110849.0000 - fp: 106162.0000 - tn: 56026848.0000 - fn: 126151.0000 - accuracy: 0.9963 - precision: 0.9829 - recall: 0.9798 - auc: 0.9994 - val_loss: 0.0117 - val_tp: 62326.0000 - val_fp: 746.0000 - val_tn: 566254.0000 - val_fn: 674.0000 - val_accuracy: 0.9977 - val_precision: 0.9882 - val_recall: 0.9893 - val_auc: 0.9997\n",
      "Epoch 8/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0145 - tp: 6121511.0000 - fp: 98183.0000 - tn: 56034784.0000 - fn: 115489.0000 - accuracy: 0.9966 - precision: 0.9842 - recall: 0.9815 - auc: 0.9994 - val_loss: 0.0111 - val_tp: 62209.0000 - val_fp: 671.0000 - val_tn: 566329.0000 - val_fn: 791.0000 - val_accuracy: 0.9977 - val_precision: 0.9893 - val_recall: 0.9874 - val_auc: 0.9998\n",
      "Epoch 9/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0138 - tp: 6126959.0000 - fp: 94287.0000 - tn: 56038712.0000 - fn: 110041.0000 - accuracy: 0.9967 - precision: 0.9848 - recall: 0.9824 - auc: 0.9995 - val_loss: 0.0098 - val_tp: 62399.0000 - val_fp: 658.0000 - val_tn: 566342.0000 - val_fn: 601.0000 - val_accuracy: 0.9980 - val_precision: 0.9896 - val_recall: 0.9905 - val_auc: 0.9998\n",
      "Epoch 10/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0129 - tp: 6135128.0000 - fp: 88361.0000 - tn: 56044704.0000 - fn: 101872.0000 - accuracy: 0.9970 - precision: 0.9858 - recall: 0.9837 - auc: 0.9995 - val_loss: 0.0092 - val_tp: 62353.0000 - val_fp: 523.0000 - val_tn: 566477.0000 - val_fn: 647.0000 - val_accuracy: 0.9981 - val_precision: 0.9917 - val_recall: 0.9897 - val_auc: 0.9998\n",
      "Epoch 11/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0123 - tp: 6139552.0000 - fp: 84586.0000 - tn: 56048396.0000 - fn: 97448.0000 - accuracy: 0.9971 - precision: 0.9864 - recall: 0.9844 - auc: 0.9995 - val_loss: 0.0086 - val_tp: 62505.0000 - val_fp: 583.0000 - val_tn: 566417.0000 - val_fn: 495.0000 - val_accuracy: 0.9983 - val_precision: 0.9908 - val_recall: 0.9921 - val_auc: 0.9999\n",
      "Epoch 12/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0119 - tp: 6142781.0000 - fp: 82047.0000 - tn: 56050972.0000 - fn: 94219.0000 - accuracy: 0.9972 - precision: 0.9868 - recall: 0.9849 - auc: 0.9995 - val_loss: 0.0093 - val_tp: 62437.0000 - val_fp: 673.0000 - val_tn: 566327.0000 - val_fn: 563.0000 - val_accuracy: 0.9980 - val_precision: 0.9893 - val_recall: 0.9911 - val_auc: 0.9998\n",
      "Epoch 13/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0115 - tp: 6146808.0000 - fp: 79055.0000 - tn: 56053924.0000 - fn: 90192.0000 - accuracy: 0.9973 - precision: 0.9873 - recall: 0.9855 - auc: 0.9996 - val_loss: 0.0087 - val_tp: 62396.0000 - val_fp: 475.0000 - val_tn: 566525.0000 - val_fn: 604.0000 - val_accuracy: 0.9983 - val_precision: 0.9924 - val_recall: 0.9904 - val_auc: 0.9999\n",
      "Epoch 14/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0113 - tp: 6148791.0000 - fp: 77540.0000 - tn: 56055480.0000 - fn: 88209.0000 - accuracy: 0.9973 - precision: 0.9875 - recall: 0.9859 - auc: 0.9996 - val_loss: 0.0074 - val_tp: 62527.0000 - val_fp: 452.0000 - val_tn: 566548.0000 - val_fn: 473.0000 - val_accuracy: 0.9985 - val_precision: 0.9928 - val_recall: 0.9925 - val_auc: 0.9999\n",
      "Epoch 15/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0109 - tp: 6152555.0000 - fp: 75117.0000 - tn: 56057916.0000 - fn: 84445.0000 - accuracy: 0.9974 - precision: 0.9879 - recall: 0.9865 - auc: 0.9996 - val_loss: 0.0086 - val_tp: 62496.0000 - val_fp: 672.0000 - val_tn: 566328.0000 - val_fn: 504.0000 - val_accuracy: 0.9981 - val_precision: 0.9894 - val_recall: 0.9920 - val_auc: 0.9998\n",
      "Epoch 16/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0106 - tp: 6155155.0000 - fp: 72739.0000 - tn: 56060264.0000 - fn: 81845.0000 - accuracy: 0.9975 - precision: 0.9883 - recall: 0.9869 - auc: 0.9996 - val_loss: 0.0072 - val_tp: 62552.0000 - val_fp: 475.0000 - val_tn: 566525.0000 - val_fn: 448.0000 - val_accuracy: 0.9985 - val_precision: 0.9925 - val_recall: 0.9929 - val_auc: 0.9999\n",
      "Epoch 17/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0104 - tp: 6156826.0000 - fp: 71897.0000 - tn: 56061064.0000 - fn: 80174.0000 - accuracy: 0.9976 - precision: 0.9885 - recall: 0.9871 - auc: 0.9996 - val_loss: 0.0072 - val_tp: 62506.0000 - val_fp: 396.0000 - val_tn: 566604.0000 - val_fn: 494.0000 - val_accuracy: 0.9986 - val_precision: 0.9937 - val_recall: 0.9922 - val_auc: 0.9999\n",
      "Epoch 18/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0102 - tp: 6159463.0000 - fp: 69517.0000 - tn: 56063488.0000 - fn: 77537.0000 - accuracy: 0.9976 - precision: 0.9888 - recall: 0.9876 - auc: 0.9996 - val_loss: 0.0076 - val_tp: 62496.0000 - val_fp: 397.0000 - val_tn: 566603.0000 - val_fn: 504.0000 - val_accuracy: 0.9986 - val_precision: 0.9937 - val_recall: 0.9920 - val_auc: 0.9999\n",
      "Epoch 19/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0102 - tp: 6160347.0000 - fp: 68527.0000 - tn: 56064508.0000 - fn: 76653.0000 - accuracy: 0.9977 - precision: 0.9890 - recall: 0.9877 - auc: 0.9996 - val_loss: 0.0073 - val_tp: 62512.0000 - val_fp: 449.0000 - val_tn: 566551.0000 - val_fn: 488.0000 - val_accuracy: 0.9985 - val_precision: 0.9929 - val_recall: 0.9923 - val_auc: 0.9999\n",
      "Epoch 20/20\n",
      "1386/1386 [==============================] - 16s 11ms/step - loss: 0.0098 - tp: 6163601.0000 - fp: 66092.0000 - tn: 56066964.0000 - fn: 73399.0000 - accuracy: 0.9978 - precision: 0.9894 - recall: 0.9882 - auc: 0.9996 - val_loss: 0.0074 - val_tp: 62494.0000 - val_fp: 437.0000 - val_tn: 566563.0000 - val_fn: 506.0000 - val_accuracy: 0.9985 - val_precision: 0.9931 - val_recall: 0.9920 - val_auc: 0.9999\n",
      "575/575 [==============================] - 3s 3ms/step - loss: 0.0132 - tp: 162651.0000 - fp: 2071.0000 - tn: 1488329.0000 - fn: 2949.0000 - accuracy: 0.9970 - precision: 0.9874 - recall: 0.9822 - auc: 0.9984\n",
      "loss = 0.013217817060649395\n",
      "TP = 162651.0\n",
      "FP = 2071.0\n",
      "TN = 1488329.0\n",
      "FN = 2949.0\n",
      "BinaryAccuracy = 0.9969686269760132\n",
      "Precision = 0.987427294254303\n",
      "Recall = 0.9821920394897461\n",
      "AUC = 0.9984005689620972\n",
      "Model: \"ResNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]                 0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 60)                   3660      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 60)                   240       ['dense[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 60)                   0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 480)                  29280     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 480)                  1920      ['dense_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 480)                  0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 480)                  0         ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 240)                  115440    ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 240)                  960       ['dense_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 240)                  0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 120)                  28920     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 120)                  480       ['dense_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 60)                   7260      ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 60)                   240       ['dense_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 60)                   0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 60)                   0         ['activation_3[0][0]',        \n",
      "                                                                     'activation[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 480)                  29280     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 480)                  1920      ['dense_5[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 480)                  0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 480)                  0         ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 240)                  115440    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 240)                  960       ['dense_6[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 240)                  0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 120)                  28920     ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 120)                  480       ['dense_7[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 60)                   7260      ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 60)                   240       ['dense_8[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 60)                   0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 60)                   0         ['activation_6[0][0]',        \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 480)                  29280     ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 480)                  1920      ['dense_9[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 480)                  0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 480)                  0         ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 240)                  115440    ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 240)                  960       ['dense_10[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 240)                  0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 120)                  28920     ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 120)                  480       ['dense_11[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 60)                   7260      ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 60)                   240       ['dense_12[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 60)                   0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 60)                   0         ['activation_9[0][0]',        \n",
      "                                                                     'add_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 480)                  29280     ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 480)                  1920      ['dense_13[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 480)                  0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 480)                  0         ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 240)                  115440    ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 240)                  960       ['dense_14[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 240)                  0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 120)                  28920     ['activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 120)                  480       ['dense_15[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 60)                   7260      ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 60)                   240       ['dense_16[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 60)                   0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 60)                   0         ['activation_12[0][0]',       \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 60)                   0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 90)                   5490      ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 747390 (2.85 MB)\n",
      "Trainable params: 740070 (2.82 MB)\n",
      "Non-trainable params: 7320 (28.59 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=metrics)\n",
    "#model.compile(optimizer=\"RMSprop\", loss=\"binary_crossentropy\", metrics=metrics)\n",
    "\n",
    "batch = 500\n",
    "epoch = 20\n",
    "\n",
    "history_1 = model.fit(Training_Data, Training_Labels, validation_split=0.01, epochs=epoch, batch_size=batch,\n",
    "                      shuffle=True, verbose=1)\n",
    "preds_1 = model.evaluate(Testing_Data, Testing_Labels)\n",
    "\n",
    "\n",
    "\n",
    "print(\"loss = \" + str(preds_1[0]))\n",
    "print(\"TP = \" + str(preds_1[1]))\n",
    "print(\"FP = \" + str(preds_1[2]))\n",
    "print(\"TN = \" + str(preds_1[3]))\n",
    "print(\"FN = \" + str(preds_1[4]))\n",
    "print(\"BinaryAccuracy = \" + str(preds_1[5]))\n",
    "print(\"Precision = \" + str(preds_1[6]))\n",
    "print(\"Recall = \" + str(preds_1[7]))\n",
    "print(\"AUC = \" + str(preds_1[8]))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d09029-6fa0-4c7c-a1c1-b079792cd02f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
